{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "import json\n",
    "from graphframes import *\n",
    "import uuid\n",
    "from graphframes.examples import Graphs\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_graphframes(graphframes):\n",
    "    print 'inside merge_graphframes: {}'.format(time.time())\n",
    "    try:\n",
    "        if(len(graphframes) > 0):\n",
    "            v = graphframes[0].vertices\n",
    "            e = graphframes[0].edges\n",
    "        for i in range(len(graphframes)):\n",
    "            if(i == 0):\n",
    "                continue;\n",
    "                \n",
    "            for i in graphframes[i].vertices.groupBy(\"id\").count():\n",
    "                #get name field of current row , may not work see stackoverflow for sol\n",
    "                name = graphframes[i].vertices[i].select(\"name\")\n",
    "                #check if it exists in v\n",
    "                existing_node = v.filter(\"name = '{}'\".format(name)).first()\n",
    "                if existing_node != None:\n",
    "                    new_vertex = sqlContext.createDataFrame([\n",
    "                                    (name, name),\n",
    "                                 ], [\"id\", \"name\"])\n",
    "                    v = v.union(new_vertex)\n",
    "                    \n",
    "            for i in graphframes[i].edges.groupBy(\"src\").count():\n",
    "                #get src field of current row - may not work see stackoverflow for sol\n",
    "                src = graphframes[i].vertices[i].select(\"src\")\n",
    "                src_id = v.filter(\"name = '{}'\".format(src)).first()\n",
    "                dst = graphframes[i].vertices[i].select(\"dst\")\n",
    "                dst_id = v.filter(\"name = '{}'\".format(dst)).first()\n",
    "                \n",
    "                new_df = sqlContext.createDataFrame([\n",
    "                            (text_name,\n",
    "                             src_id,\n",
    "                             dst_id,\n",
    "                             rel[i],\n",
    "                             temporal.get(sub[i] + rel[i] + obj[i], \"None\"), \n",
    "                             spatial.get(sub[i] + rel[i] + obj[i], \"None\"), \n",
    "                             purpose.get(sub[i] + rel[i] + obj[i], \"None\")) \n",
    "                         ], [\"text_name\", \"src\", \"dst\", \"predicate\", \"temporal\", \"spatial\", \"purpose\"])\n",
    "                e = e.union(new_df)\n",
    "                    \n",
    "        print 'Safely exiting merge_graphframes {}'.format(time.time())\n",
    "        return GraphFrame(v, e)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graphframe(graphene_output, text_name):\n",
    "    print 'inside make_graphframe {} {}'.format(text_name, time.time())\n",
    "    try:\n",
    "        \n",
    "        json_obj = json.loads(graphene_output.replace(\"'\", '\\\"'))\n",
    "\n",
    "        sentences = json_obj[\"sentences\"]\n",
    "\n",
    "        num_sentences = len(sentences)\n",
    "\n",
    "        sub = []\n",
    "        rel = []\n",
    "        obj = []\n",
    "        temporal = {}\n",
    "        purpose = {}\n",
    "        spatial = {}\n",
    "\n",
    "        for i in range(num_sentences):\n",
    "            sentence = sentences[i]\n",
    "            extractionMap = sentence[\"extractionMap\"]\n",
    "            for key, val in extractionMap.items():\n",
    "                sub.append(val[\"arg1\"] if val[\"arg1\"] else 'None')\n",
    "                rel.append(val[\"relation\"] if val[\"relation\"] else 'None')\n",
    "                obj.append(val[\"arg2\"] if val[\"arg2\"] else 'None')\n",
    "                simpleContexts = val[\"simpleContexts\"]\n",
    "                num_contexts = len(simpleContexts)\n",
    "                for j in range(num_contexts):\n",
    "                    simpleContext = simpleContexts[j]\n",
    "                    classification = simpleContext[\"classification\"]\n",
    "                    key = (val[\"arg1\"] + val[\"relation\"] + val[\"arg2\"])\n",
    "                    if (classification == 'TEMPORAL'):\n",
    "                        temporal.update({val[\"arg1\"] + val[\"relation\"] + val[\"arg2\"]:simpleContext[\"text\"]})  #save in a Temporal hashmap like [\"Obamaispresident\"] : [\"from 2017 to 2019\"]\n",
    "                    elif (classification == 'PURPOSE'):\n",
    "                        purpose.update({val[\"arg1\"] + val[\"relation\"] + val[\"arg2\"]:simpleContext[\"text\"]})  #save in a Temporal hashmap like [\"Obamaispresident\"] : [\"from 2017 to 2019\"]\n",
    "                    elif (classification == 'SPATIAL'):\n",
    "                        spatial.update({val[\"arg1\"] + val[\"relation\"] + val[\"arg2\"]:simpleContext[\"text\"]})  #save in a Temporal hashmap like [\"Obamaispresident\"] : [\"from 2017 to 2019\"] \n",
    "        \n",
    "        print len(temporal)\n",
    "        print len(spatial)\n",
    "        print len(purpose)\n",
    "        \n",
    "        for i in xrange(len(sub)):\n",
    "            \n",
    "            if i == 0:\n",
    "                v = sqlContext.createDataFrame([\n",
    "                        (sub[i], sub[i]),\n",
    "                        (obj[i], obj[i]),\n",
    "                    ], [\"id\", \"name\"])\n",
    "\n",
    "                e = sqlContext.createDataFrame([\n",
    "                        (text_name,\n",
    "                         sub[i],\n",
    "                         obj[i], \n",
    "                         rel[i], \n",
    "                         temporal.get(sub[i] + rel[i] + obj[i], \"None\"), \n",
    "                         spatial.get(sub[i] + rel[i] + obj[i], \"None\"), \n",
    "                         purpose.get(sub[i] + rel[i] + obj[i], \"None\"))\n",
    "                    ], [\"article_name\", \"src\", \"dst\", \"predicate\", \"temporal\", \"spatial\", \"purpose\"])\n",
    "            else:\n",
    "\n",
    "                ########### Put sub[i] in vertex table if not present ################\n",
    "                \n",
    "                #See if already exist\n",
    "                existing_sub = v.filter(\"name = '{}'\".format(sub[i])).first()\n",
    "\n",
    "                if existing_sub != None:\n",
    "                    #sub[i] already exists\n",
    "                    sub_id = None\n",
    "                else:\n",
    "                    sub_id = sub[i]\n",
    "\n",
    "                if sub_id != None:\n",
    "                    new_vertex = sqlContext.createDataFrame([\n",
    "                                    (sub_id, sub[i]),\n",
    "                                 ], [\"id\", \"name\"])\n",
    "                    v = v.union(new_vertex)\n",
    "\n",
    "                ########### Put obj[i] in vertex table if not present ################\n",
    "\n",
    "                #See if already exist\n",
    "                existing_obj = v.filter(\"name = '{}'\".format(obj[i])).first()\n",
    "\n",
    "                if existing_obj != None:\n",
    "                    #obj[i] already exists\n",
    "                    obj_id = None\n",
    "                else:\n",
    "                    obj_id = obj[i]\n",
    "\n",
    "\n",
    "                if obj_id != None:\n",
    "                    new_vertex = sqlContext.createDataFrame([\n",
    "                                    (obj_id, obj[i]),\n",
    "                                 ], [\"id\", \"name\"])\n",
    "                    v = v.union(new_vertex)\n",
    "\n",
    "                ############# Put entry in edge frame #########################\n",
    "\n",
    "                sub_id = v.filter(\"name = '{}'\".format(sub[i])).first().id\n",
    "                obj_id = v.filter(\"name = '{}'\".format(obj[i])).first().id\n",
    "                \n",
    "                new_df = sqlContext.createDataFrame([\n",
    "                            (text_name,\n",
    "                             sub_id,\n",
    "                             obj_id,\n",
    "                             rel[i],\n",
    "                             temporal.get(sub[i] + rel[i] + obj[i], \"None\"), \n",
    "                             spatial.get(sub[i] + rel[i] + obj[i], \"None\"), \n",
    "                             purpose.get(sub[i] + rel[i] + obj[i], \"None\")) \n",
    "                         ], [\"text_name\", \"src\", \"dst\", \"predicate\", \"temporal\", \"spatial\", \"purpose\"])\n",
    "                e = e.union(new_df)\n",
    "        print 'safely exiting make_graphframe {} {}'.format(text_name, time.time())    \n",
    "        return GraphFrame(v, e)\n",
    "    except Exception as e:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "def get_graphene_response(file):\n",
    "    print '#################  Inside get_graphene_response {} {}'.format(file, time.time())\n",
    "    try:\n",
    "        \n",
    "        \n",
    "        cwd = os.getcwd()\n",
    "        f=open( cwd + \"/dataset/1000words/\" + file + \".txt\", \"r\")\n",
    "        \n",
    "        content = f.read()\n",
    "        content = ''.join(e for e in content if (e.isalnum() or e == ' ' or e == '.'))\n",
    "        payload = {\"doCoreference\": \"false\", \"isolateSentences\": \"false\"}\n",
    "        payload['text'] = content\n",
    "        headers = {\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"}\n",
    "        \n",
    "        session = requests.Session()\n",
    "        retry = Retry(connect=3, backoff_factor=0.5)\n",
    "        adapter = HTTPAdapter(max_retries=3)\n",
    "        session.mount('http://', adapter)\n",
    "        url = 'http://34.96.81.129/relationExtraction/text' #container-native\n",
    "        \n",
    "        response = session.post(url, data=json.dumps(payload), headers=headers, timeout=2000)\n",
    "        while(response.status_code!=200):\n",
    "            response = session.post(url, data=json.dumps(payload), headers=headers, timeout=2000)\n",
    "            \n",
    "        print '###########  Safely now exiting get_graphene_response {} {}'.format(file, time.time())\n",
    "        print response\n",
    "        \n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print  e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knowledge_graph(article_dict):\n",
    "    graphframes = []\n",
    "    for key,value in article_dict.iteritems():\n",
    "        graphene_response = get_graphene_response(value)\n",
    "        graphframes.append(make_graphframe(graphene_response, key))\n",
    "        \n",
    "    return merge_graphframes(graphframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_result(graphframe, conditions = []):\n",
    "    filtered = graphframe.edges\n",
    "    for x in conditions:\n",
    "        print x\n",
    "        filtered = filtered.filter(x)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_columns(filtered_result, columns = []):\n",
    "    selected = filtered_result\n",
    "    selected = selected.select(columns)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_result(graphframe, conditions = []):\n",
    "    filtered = graphframe.edges\n",
    "    vertices = graphframe.vertices\n",
    "    for x in conditions:\n",
    "        str = x.split()\n",
    "        col = str[0]\n",
    "        if (col == 'src'):\n",
    "            x2 = x.replace('src', 'name')\n",
    "            vertices.filter(x2)\n",
    "            joinExp = vertices[\"id\"] == filtered[\"src\"]\n",
    "            filtered= filtered.join(vertices, joinExp, 'inner')\n",
    "        elif (col == 'dst'):\n",
    "            x2 = x.replace('dst', 'name')\n",
    "            vertices.filter(x2)\n",
    "            joinExp = vertices[\"id\"] == filtered[\"dst\"]\n",
    "            filtered= filtered.join(vertices, joinExp, 'inner')\n",
    "        else:\n",
    "            filtered = filtered.filter(x)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_columns(filtered_result, columns = []):\n",
    "    cols = []\n",
    "    for i in range(len(columns)):\n",
    "        if(columns[i].strip() == ('s')):\n",
    "            cols.append(\"src\")\n",
    "        if(columns[i].strip() == ('p')):\n",
    "            cols.append(\"predicate\")\n",
    "        if(columns[i].strip() == ('o')):\n",
    "            cols.append(\"dst\")\n",
    "        if(columns[i].strip() == ('temporal')):\n",
    "            cols.append(\"temporal\")\n",
    "        if(columns[i].strip() == ('spatial')):\n",
    "            cols.append(\"spatial\")\n",
    "        if(columns[i].strip() == ('purpose')):\n",
    "            cols.append(\"purpose\")\n",
    "        if(columns[i].strip() == ('*')):\n",
    "            cols.append(\"*\")\n",
    "    selected = filtered_result\n",
    "    selected = selected.select(cols)\n",
    "    return selected\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1567023024.19\n",
      "*\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'graphframes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3c5203a11a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#if(graph == 'FROM'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#    query_string += \",article_name like '%{}%'\".format(graphframe, graphframe)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graphframes' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "query_string1 = \"\"\"*\n",
    "                 src ~Facebook\n",
    "                 predicate ~founded\"\"\"\n",
    "\n",
    "print time.time()\n",
    "\n",
    "conditions = []\n",
    "split_query = query_string1.splitlines()\n",
    "if '*' in split_query:\n",
    "    select_cols = '*'\n",
    "else:    \n",
    "    select_cols = split_query[0].strip().split(',')\n",
    "\n",
    "    \n",
    "graph = graphframes[0]\n",
    "    \n",
    "for i in range(len(split_query)):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    c = split_query[i].strip().split(' ')\n",
    "    \n",
    "    col_name = c[0]\n",
    "    if(c[1] == \"=\"):\n",
    "        conditions += (col_name + '=' + \"'\" + c[2] + \"'\")\n",
    "    elif c[1].find('~')>-1:\n",
    "        val = c[1].split('~')\n",
    "        if len(c)>2:\n",
    "            val[1] = val[1]+' '+ c[2]+' '+c[3]\n",
    "        conditions.append(col_name + ' like ' + \"'%\" + val[1] + \"%'\")\n",
    "        \n",
    "filtered_result = get_filtered_result(graph, conditions)\n",
    "final_output = get_selected_columns( filtered_result, select_cols)\n",
    "\n",
    "print time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
